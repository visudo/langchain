{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35130609-b765-41f4-b5c4-3302bed8198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0.0,api_key=os.environ['OPENAI_API_KEY'],base_url=\"https://api-inference.modelscope.cn/v1/\", http_client=httpx.Client(verify=False),model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303dc48-6412-411c-bd62-44e808ef07b6",
   "metadata": {},
   "source": [
    "#### 1. ConversationBufferMemory（对话缓存记忆）\n",
    "这种记忆允许存储消息，然后从变量中提取消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c68745c-9291-4685-bc82-2f9e595fc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory = memory,\n",
    "                                 verbose=True#查看Langchain实际上在做什么，设为FALSE的话只给出回答，看到不到下面绿色的内容\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613b455f-03cc-4492-a78c-d076f490b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is andy\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, Andy! How can I assist you today? 😊'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is andy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83a2475-525d-459d-a07c-42019217461b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is andy\n",
      "AI: Hello, Andy! How can I assist you today? 😊\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 1 plus 1 is 2. \\n\\n**Answer:** 1 + 1 equals 2.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef54468-2a59-49ef-ab71-2bbe26c5bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is andy\n",
      "AI: Hello, Andy! How can I assist you today? 😊\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1 plus 1 is 2. \n",
      "\n",
      "**Answer:** 1 + 1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619f0c50-2673-4d15-9015-0883e316b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is andy\\nAI: Hello, Andy! How can I assist you today? 😊\\nHuman: What is 1+1?\\nAI: The sum of 1 plus 1 is 2. \\n\\n**Answer:** 1 + 1 equals 2.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f633cbf-e570-4fcb-852d-127301e72084",
   "metadata": {},
   "source": [
    "#### 2. ConversationBufferWindowMemory（对话缓存窗口记忆）\n",
    "随着对话变得越来越长，所需的内存量也变得非常长。将大量的tokens发送到LLM的成本，也会变得更加昂贵,这也就是为什么API的调用费用，通常是基于它需要处理的tokens数量而收费的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "190b1693-e87c-41a3-8fe7-f028a092e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)     # k=1表明只保留一个对话记忆    \n",
    "conversation = ConversationChain(llm=llm, memory = memory,\n",
    "    verbose=True    #这里改为FALSE不显示提示，你可以尝试修改为TRUE后的结果\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37114b47-95fe-48e9-94b3-287327a624c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is andy\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, Andy! How can I assist you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is andy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0492917-12aa-4f5e-9d44-61bd63209a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is andy\n",
      "AI: Hello, Andy! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 1 and 1 is 2. \\n\\n**Step-by-Step Explanation:**\\n\\n1. Start with the number 1.\\n2. Add another 1 to it.\\n3. The total is 2.\\n\\nThus, 1 + 1 equals 2.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a9dbdc-6443-4863-aa04-d6672ff68a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1 and 1 is 2. \n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. Start with the number 1.\n",
      "2. Add another 1 to it.\n",
      "3. The total is 2.\n",
      "\n",
      "Thus, 1 + 1 equals 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have a name, but I'm here to help you with whatever you need!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a11ead-ba94-4d77-91dc-9803b5043a9a",
   "metadata": {},
   "source": [
    "#### 3. ConversationTokenBufferMemory（对话token缓存记忆）\n",
    "使用对话token缓存记忆，内存将限制保存的token数量。如果token数量超出指定数目，它会切掉这个对话的早期部分 以保留与最近的交流相对应的token数量，但不超过token限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0daaffa0-081b-4c08-9f27-a0d8ba5aa5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e76d49-f2e3-4050-9624-111eb689f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0.0,api_key=os.environ['OPENAI_API_KEY'],base_url=\"https://api-inference.modelscope.cn/v1/\", http_client=httpx.Client(verify=False),model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8034c6fd-9a9f-4fc2-b22e-0fdb8739278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10 # 限制最大10个token\n",
    "                                      )\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddab6e12-cad6-4ab1-8c7d-e5a6eddb2e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Charming!'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41581d-2e7d-4d45-b44b-943d6e967628",
   "metadata": {},
   "source": [
    "#### 4. ConversationSummaryBufferMemory（对话摘要缓存记忆）\n",
    "这种Memory的想法是，不是将内存限制为基于最近对话的固定数量的token或固定数量的对话次数窗口，而是使用LLM编写到目前为止历史对话的摘要，并将其保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1732aa32-e5d9-458b-93e5-b6d92c8019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)   #使用对话摘要缓存记忆\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f4d0d8-69ba-4949-87b1-f0617a00719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human asks what the AI thinks of artificial intelligence. The AI thinks AI is a force for good because it will help humans reach their full potential. The human greets the AI, they exchange casual greetings, and the human asks about the AI\\'s schedule. The AI responds with \"Cool.\"\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d1bd69-06f0-4565-8a3f-56b4369c432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed01f1f-c426-4012-8d5f-528955c15201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human asks what the AI thinks of artificial intelligence. The AI thinks AI is a force for good because it will help humans reach their full potential. The human greets the AI, they exchange casual greetings, and the human asks about the AI's schedule. The AI responds with \"Cool.\"\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That sounds like a fantastic idea! A fun and interactive demo would definitely make a great impression. How about setting up a mini \"AI assistent\" game or challenge at the restaurant? You could create a fun, interactive experience where the AI demonstrates its problem-solving skills, like playing a quick game of 20 Questions, or helping with a fun trivia quiz. Alternatively, you could showcase an AI-powered interactive art installation or a creative project that brings AI to life in a playful way. The key is to make it engaging and show how AI can add value to everyday experiences. What do you think?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a81342bd-51cf-4ed8-97b9-6e91c16ac20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human asks what the AI thinks of artificial intelligence. The AI thinks AI is a force for good because it will help humans reach their full potential. The human greets the AI, they exchange casual greetings, and the human asks about the AI\\'s schedule. The AI responds with \"Cool.\" The AI provides a detailed schedule: a meeting at 8am with the product team, 9am-12pm working on the LangChain project, a noon lunch at an Italian restaurant with a customer, and a demo idea for a fun and interactive experience like a mini \"AI assistant\" game or an AI-powered interactive art installation. The human asks for a good demo, and the AI suggests ideas like a game of 20 Questions or a trivia quiz to showcase AI\\'s problem-solving skills.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})   #摘要记录更新了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043103d-fafd-4a5f-b66b-e7f4aca8a0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
